{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import tensorflow as tf\n",
    "import prelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\art\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\art\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:107: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\art\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:1113: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\art\\lib\\site-packages\\keras\\src\\saving\\legacy\\saved_model\\load.py:1113: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\art\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Dell\\anaconda3\\envs\\art\\lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the FaceNet model\n",
    "facenet_model = tf.saved_model.load('facenet_saved_model')\n",
    "\n",
    "model_path = \"pnet_model\"\n",
    "# Define custom objects dictionary with custom layers\n",
    "custom_objects = {'PReLU': prelu.PReLU}\n",
    "\n",
    "# Load the model with custom objects\n",
    "loaded_model = tf.keras.models.load_model(model_path, custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to preprocess an image\n",
    "def preprocess_image(image):\n",
    "    # Convert image to RGB\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Resize image to a standard size\n",
    "    resized = cv2.resize(image_rgb, (128, 128))\n",
    "    # Convert the image to float32\n",
    "    resized_float32 = resized.astype('float32') / 255.0\n",
    "    return resized_float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to classify a face using FaceNet embeddings and cosine similarity\n",
    "def classify_face(image, known_embeddings, threshold=0.5):\n",
    "    # Extract embedding from the image using FaceNet\n",
    "    embedding = extract_embedding(image)\n",
    "    if embedding is not None:\n",
    "        # Calculate cosine similarity between the embedding and known embeddings\n",
    "        similarities = [1 - cosine(embedding, known_embedding) for known_embedding in known_embeddings]\n",
    "        # Get the index of the highest similarity\n",
    "        max_index = np.argmax(similarities)\n",
    "        # Check if the similarity is above the threshold\n",
    "        if similarities[max_index] > threshold:\n",
    "            # Return the index corresponding to the highest similarity\n",
    "            return max_index\n",
    "        else:\n",
    "            # If similarity is below threshold, return None\n",
    "            return None\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to get embedding using FaceNet model\n",
    "def get_embedding(model, face):\n",
    "    face = face.astype('float32')\n",
    "    mean, std = face.mean(), face.std()\n",
    "    face = (face - mean) / std\n",
    "    sample = np.expand_dims(face, axis=0)\n",
    "    # Convert the input to a TensorFlow tensor\n",
    "    input_tensor = tf.convert_to_tensor(sample)\n",
    "    # Perform inference using the model\n",
    "    embedding = model.signatures[\"serving_default\"](input_tensor)\n",
    "    # Convert the embedding to a numpy array\n",
    "    embedding = embedding['Bottleneck_BatchNorm'].numpy()\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract embeddings using FaceNet\n",
    "# Function to extract embeddings using FaceNet\n",
    "def extract_embedding(image):\n",
    "    # Use the loaded_model for face detection\n",
    "    faces = loaded_model.predict(np.expand_dims(image, axis=0))\n",
    "    print(faces['bbox_output'][0])\n",
    "    # if len(faces) > 0 and faces[0] is not None:\n",
    "    # Assuming only one face is detected, consider the first one\n",
    "    face = faces['bbox_output'][0]\n",
    "    # Assuming the output of the model is [x, y, width, height]\n",
    "    x, y, width, height = face\n",
    "    # Crop face from the image\n",
    "    face_image = image[int(y):int(y+height), int(x):int(x+width)]\n",
    "    # Preprocess the face for FaceNet\n",
    "    preprocessed_face = preprocess_image(face_image)\n",
    "    # Get embedding using FaceNet\n",
    "    embedding = get_embedding(facenet_model, preprocessed_face)\n",
    "    return embedding\n",
    "    # else:\n",
    "    #     return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 51ms/step\n",
      "[0.32725662 0.24495795 1.0536971  0.88735235]\n",
      "Recognized names:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Function to draw bounding box and label\n",
    "def draw_box(image, box, label):\n",
    "    x, y, w, h = box\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "    cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
    "\n",
    "# Load known embeddings and labels\n",
    "known_data = np.load(\"train_data_with_facenet.npz\")\n",
    "known_embeddings = known_data['embeddings']\n",
    "known_labels = known_data['labels']\n",
    "\n",
    "# Load the input image\n",
    "input_image = cv2.imread(\"new/dhukha/Abdullah_0003.13.jpg\")\n",
    "\n",
    "# Preprocess the input image\n",
    "preprocessed_image = preprocess_image(input_image)\n",
    "\n",
    "recognized_names = []  # To store recognized names\n",
    "\n",
    "# Extract embeddings and classify faces in the input image\n",
    "label_index = classify_face(preprocessed_image, known_embeddings)\n",
    " print('hello world outside')\n",
    "if label_index is not None:\n",
    "    print('hello world')\n",
    "    # Get the label corresponding to the index\n",
    "    label = known_labels[label_index]\n",
    "    recognized_names.append(label)\n",
    "    # Draw bounding box and label\n",
    "    draw_box(input_image, (0, 0, preprocessed_image.shape[1], preprocessed_image.shape[0]), label)\n",
    "\n",
    "# Show the result\n",
    "cv2.imshow('Face Recognition Result', input_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print recognized names\n",
    "print(\"Recognized names:\")\n",
    "for name in recognized_names:\n",
    "    print(name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "art",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
